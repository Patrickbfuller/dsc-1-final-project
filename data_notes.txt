Data Notes:
    PRICE(whats the best mse we can get with a model from each?)
price: 

log price:

    PREDICTORS(initial review from raw multivar regresn)
bedrooms:  
            -Rubbish
            -indicating value = 1 for presumable studios
            , neg corr on raw multivar regression???
            thinking  THROW OUT
Bathrooms:
            -A team
sqft_liv: 
            -A team
            -little skewed,  good corr -> prob log norm  

sqft_lot:  -Rubbish
            -indicating size of apt buildings
            ,     - THROW OUT - 

floors:
            -Rubbish
            -wild p val when raw

wtrft: 
            - A team
            -hi coef, lo p
            -not very correlated with other predictors 
            
view:
            
            -hi coef, lo p
            , "wouldn't nicer places just be viewed more??
                How could it cause a higher price??
                Could deter lo-balling"
                
condxn:
            - Rubbish
            -grade seems to be a better indicator
            , and they're refering to the same thing rigght?

grade:
            - A team. v.corr()
            -seems to be better indicator than condxn
            
sqft_above:
            - Put on the B team
            -not as strong as sqft_liv and they're covering 
                the same thing
            , maybe use a ratio of 'what portion of sqft is
                above the basement??'
                
sqft_base:
            B -team
            -similar thoughts
            , also some 'question mark' values
            one-hot for has basement? or not
            
yr_built:
            -neg coef in model??
            , maybe combine with yr_reno'd
            
yr_reno'd:
            -turn into time since repair
            
zip:
            -bool for 'in seattle'?
            
lat&long:
            -compute distance from seattle center
            
living15:
           -A team
           -v corr()
lot15:
            -Rubbish
B concepts: 

From the top:

Data Preparation:

1. Partition raw data with sklearn train_test_split; 15% does to df_holdout
2. Look at scatter plots of each predictor with respect to "price"
3. Remove columns "id," "date," and "sqft_basement,"; not all of the dwellings have basements.
4. Scatter matrix and correlation table of all remaining categories (except zipcode) to consider which had a slightly linear correlation. Of these, "sqft_living," ""sqft_living15," "bathrooms," and "grade" were the best candidates. However, the distributions were not very normal. Ultimately, the model improved using "log_price," "log_sqft_living," "log_sqft_living15," "grade," and bathrooms. The distribution of bathrooms did not improve with log transform because some of its values are less than one.
5. One-Hot-Encoding of "zipcode." 

Modeling:
    With these columns to choose from we modeled several combinations to for multilinear regression with the aim of minimizing Mean Absolute Error. We built a function, feat_to_model_kfold_eval, which takes as arguments target = "column name," predictors = list of column names, df = dataframe, kvals = list of k-folds, show_summary = Boolean, price_logged = Boolean, MAE = Boolean. This returns the "best fit" line array with intercept and coefficients, total errors, MAE (in dollars) if True and RMSE if False. This enabled us to rapidly run, cross-validate, and compare models. 

Feature Engineering:
    Downtown Seattle is a major area of employment, so we used the Haversine to generate distance_to_flatiron as a new column for consideration. Including it lowered our MAE somewhat, but it's worth noting that downtown Seattle is on the water; preventing an even distribution of distances.
    
Conclusion:
    The most robust predictors were "log_grade," "log_sqft_living," "log_sqft_living15," and "bathrooms." 

